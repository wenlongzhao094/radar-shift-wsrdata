"""
This script is a modifiable template which is written for dataset v0.2.B,
where B indicates different sampling strategies.

It reads the json generated by organize_screened_csv_as_json.py in the format of
station_years[station_year] = {
    'all_scans_with_check':     {},  # scan: {"avg_dbz": float, "dualpol": True/False}                  SCAN
    'all_days_to_scans':        {},  # day: set(scan)                                                   DAY

    'n_roost_annotations':              0,                                                              ANN
    'n_roost_annotations_not_miss_day': 0,                                                              ANN
    'n_bad_track_annotations':          0,                                                              ANN
    'scans_with_roosts':                set(),  # positive scans                                        SCAN
    'roost_days':                       set(),  # days with roosts                                      DAY

    'n_scans_without_roosts_in_roost_days': 0,  # negative scans                                        SCAN
    'n_scans_in_non_roost_days':            0,  # scans from sampled non_roost_days become negatives    SCAN
    'non_roost_days':                       set(),  # days without roosts                               DAY
}
and defines scan lists and splits under **static/scan_lists/v0.2.0**.

We define splits by randomly sampling station-days.
1. no_dualpol: scans without dualpol, mostly pre-2013 with legacy resolution
2. dualpol: scans with dualpol, mostly post-2013 with super-resolution
3. standard: no-dualpol + dualpol

We also define splits by stations,
omitting KMQT in station splits since it has almost no roosts.
4. station1
    - train: East stations KTYX 4111 dets, KBUF 8254, KCLE 14619, KDTX 10956, KAPX 1094
    - val: Middle stations KIWX 7865, KGRR 5498
    - test: West stations KLOT 2067, KGRB 5125, KMKX 2531, KDLH 2491
5. station2
    - train: East stations KTYX 4111 dets, KBUF 8254, KCLE 14619, KDTX 10956, KAPX 1094
    - val: West stations KLOT 2067, KGRB 5125, KMKX 2531, KDLH 2491
    - test: Middle stations KIWX 7865, KGRR 5498
6. station3
    - train: South stations KIWX 7865, KLOT 2067, KCLE 13619, KMKX 2531, KGRR 5498, KDTX 10956
    - val: Middle stations KBUF 8254, KGRB 5125
    - test: North stations KTYX 4111, KAPX 1094, KDLH 2491
7. station4
    - train: South stations KIWX 7865, KLOT 2067, KCLE 13619, KMKX 2531, KGRR 5498, KDTX 10956
    - val: North stations KTYX 4111, KAPX 1094, KDLH 2491
    - test: Middle stations KBUF 8254, KGRB 5125
"""

import numpy as np
import json, os
import random

STATIONS = [
    'KAPX', 'KBUF', 'KCLE', 'KDLH', 'KDTX', 'KGRB',
    'KGRR', 'KIWX', 'KLOT', 'KMKX', 'KMQT', 'KTYX',
]
DATASET_VERSION = 'v0.2.5' # TODO
DATASET_VERSION_TO_RATIOS = {
    'v0.2.0': [
        1.0, # number of non_roost_day / number of roost_day
        0.5, # percentage of non_roost_day from top quartile dbz instead of the rest
    ],
    'v0.2.1': [0.8, 0.5],
    'v0.2.2': [0.6, 0.5],
    'v0.2.3': [0.4, 0.5],
    'v0.2.4': [0.6, 0.3],
    'v0.2.5': [0.6, 0.7],
    # TODO
}
SCAN_LIST_DIR = f'../static/scan_lists/{DATASET_VERSION}'
STATION_SPLIT_NAMES = ["station1", "station2", "station3", "station4"]
SPLIT_NAMES = ["no_dualpol", "dualpol", "standard"] + STATION_SPLIT_NAMES
STATION_SPLITS = {
    "station1": {
        "train": ["KTYX", "KBUF", "KCLE", "KDTX", "KAPX"],
        "val": ["KIWX", "KGRR"],
        "test": ["KLOT", "KGRB", "KMKX", "KDLH"]
    },
    "station2": {
        "train": ["KTYX", "KBUF", "KCLE", "KDTX", "KAPX"],
        "val": ["KLOT", "KGRB", "KMKX", "KDLH"],
        "test": ["KIWX", "KGRR"]
    },
    "station3": {
        "train": ["KIWX", "KLOT", "KCLE", "KMKX", "KGRR", "KDTX"],
        "val": ["KBUF", "KGRB"],
        "test": ["KTYX", "KAPX", "KDLH"]
    },
    "station4": {
        "train": ["KIWX", "KLOT", "KCLE", "KMKX", "KGRR", "KDTX"],
        "val": ["KTYX", "KAPX", "KDLH"],
        "test": ["KBUF", "KGRB"]
    },
}
SPLITS = {
    split_name: {
        "dir": f'../static/scan_lists/{DATASET_VERSION}/{DATASET_VERSION}_{split_name}_splits',
        "train_scans": [],
        "val_scans": [],
        "test_scans": [],
        "n_train_days": 0,
        "n_val_days": 0,
        "n_test_days": 0,
    } for split_name in SPLIT_NAMES
}
TRAIN_RATIO = 4/7.
VAL_RATIO = 1/7.

for split_name in SPLITS:
    os.makedirs(SPLITS[split_name]["dir"], exist_ok=True)
logs = []

# Collect all station years
station_years = {}
for station in STATIONS:
    station = json.load(open(f'prepare_dataset_v0.2.0_help/all_days_all_scans_{station}.json', 'r'))
    for station_year in station:
        station_years[station_year] = station[station_year]

# Print some stats
logs.append(str(DATASET_VERSION) + '\n\n')

logs.append(f'n_days: {sum([len(station_years[sy]["all_days_to_scans"]) for sy in station_years])}\n')
logs.append(f'n_roost_days: {sum([len(station_years[sy]["roost_days"]) for sy in station_years])}\n')
logs.append(f'n_non_roost_days: {sum([len(station_years[sy]["non_roost_days"]) for sy in station_years])}\n\n')

logs.append(f'n_scans: {sum([len(station_years[sy]["all_scans_with_check"]) for sy in station_years])}\n')
n_scans_with_roosts = sum([len(station_years[sy]["scans_with_roosts"]) for sy in station_years])
logs.append(f'n_scans_with_roosts: {n_scans_with_roosts}\n')
logs.append(f'n_scans_without_roosts_in_roost_days: '
            f'{sum([station_years[sy]["n_scans_without_roosts_in_roost_days"] for sy in station_years])}\n')
logs.append(f'n_scans_in_non_roost_days: '
            f'{sum([station_years[sy]["n_scans_in_non_roost_days"] for sy in station_years])}\n\n')

logs.append(f'n_roost_annotations: '
            f'{sum([station_years[sy]["n_roost_annotations"] for sy in station_years])}\n')
logs.append(f'n_roost_annotations_not_miss_day: '
            f'{sum([station_years[sy]["n_roost_annotations_not_miss_day"] for sy in station_years])}\n')
logs.append(f'n_bad_track_annotations: '
            f'{sum([station_years[sy]["n_bad_track_annotations"] for sy in station_years])}\n\n')

# Create the scan list txt for the dataset
with open(os.path.join(SCAN_LIST_DIR, 'scan_list.txt'), "w") as f:
    for station_year in station_years:
        f.writelines([scan + '\n' for scan in station_years[station_year]['all_scans_with_check']])

# Create splits
random.seed(1)
for station_year in station_years:
    # Preparation for no_dualpol, dualpol, and standard splits
    train_days = []
    val_days = []
    test_days = []
    dualpol_days = set()
    for scan in station_years[station_year]['all_scans_with_check']:
        if station_years[station_year]['all_scans_with_check'][scan]['dualpol']:
            dualpol_days.add(scan[4:12])

    # Positive days
    roost_days = list(station_years[station_year]['roost_days'])
    random.shuffle(roost_days)
    _n_pos = len(roost_days)
    _n_train = int(TRAIN_RATIO * _n_pos)
    _n_val = int(VAL_RATIO * _n_pos)
    train_days.extend(roost_days[:_n_train])
    val_days.extend(roost_days[_n_train:_n_train+_n_val])
    test_days.extend(roost_days[_n_train+_n_val:_n_pos])

    # Negative days
    non_roost_days = list(station_years[station_year]['non_roost_days'])
    dbz = [np.mean([
        station_years[station_year]['all_scans_with_check'][scan]['avg_dbz']
        for scan in station_years[station_year]['all_days_to_scans'][non_roost_day]
    ]) for non_roost_day in non_roost_days]
    non_roost_days = [non_roost_day for non_roost_day, _ in sorted(zip(non_roost_days, dbz), key=lambda p: -p[1])]
    _n_neg = _n_pos * DATASET_VERSION_TO_RATIOS[DATASET_VERSION][0]

    # negative days whose lowest elevation day average dbz are in the top quartile
    top_dbz_non_roost_days = non_roost_days[:int(len(non_roost_days)/4)]
    random.shuffle(top_dbz_non_roost_days)
    _n_dbz_neg = min(len(top_dbz_non_roost_days), int(_n_neg * DATASET_VERSION_TO_RATIOS[DATASET_VERSION][1]))
    _n_train = int(TRAIN_RATIO * _n_dbz_neg)
    _n_val = int(VAL_RATIO * _n_dbz_neg)
    train_days.extend(top_dbz_non_roost_days[:_n_train])
    val_days.extend(top_dbz_non_roost_days[_n_train:_n_train+_n_val])
    test_days.extend(top_dbz_non_roost_days[_n_train+_n_val:_n_dbz_neg])

    # negative days from the rest
    non_roost_days = non_roost_days[int(len(non_roost_days)/4):]
    random.shuffle(non_roost_days)
    _n_rand_neg = min(len(non_roost_days), int(_n_neg * (1 - DATASET_VERSION_TO_RATIOS[DATASET_VERSION][1])))
    _n_train = int(TRAIN_RATIO * _n_rand_neg)
    _n_val = int(VAL_RATIO * _n_rand_neg)
    train_days.extend(non_roost_days[:_n_train])
    val_days.extend(non_roost_days[_n_train:_n_train+_n_val])
    test_days.extend(non_roost_days[_n_train+_n_val:_n_rand_neg])

    # Create splits
    def update_SPLITS(split_name, split, days):
        SPLITS[split_name][f"n_{split}_days"] += len(days)
        for day in days:
            SPLITS[split_name][f"{split}_scans"].extend(station_years[station_year]["all_days_to_scans"][day])

    # no_dualpol, dualpol, standard splits
    def splits_helper(split, split_days):
        no_dualpol_split_days = sorted(list(split_days.difference(dualpol_days)))
        update_SPLITS("no_dualpol", split, no_dualpol_split_days)
        dualpol_split_days = sorted(list(split_days.intersection(dualpol_days)))
        update_SPLITS("dualpol", split, dualpol_split_days)
        split_days = sorted(list(split_days))
        update_SPLITS("standard", split, split_days)
    splits_helper("train", set(train_days))
    splits_helper("val", set(val_days))
    splits_helper("test", set(test_days))

    # station splits
    all_days = sorted(train_days + val_days + test_days)
    for station_split in STATION_SPLITS:
        station = station_year.split('_')[0]
        for split in ["train", "val", "test"]:
            if station in STATION_SPLITS[station_split][split]:
                update_SPLITS(station_split, split, all_days)

for split_name in SPLITS:
    logs.append(f"{DATASET_VERSION}_{split_name}_splits\n")
    train_scans = set(SPLITS[split_name]["train_scans"])
    val_scans = set(SPLITS[split_name]["val_scans"])
    test_scans = set(SPLITS[split_name]["test_scans"])
    assert not train_scans.intersection(val_scans)
    assert not train_scans.intersection(test_scans)
    assert not val_scans.intersection(test_scans)
    train_scans = [scan + '\n' for scan in sorted(list(train_scans))]
    val_scans = [scan + '\n' for scan in sorted(list(val_scans))]
    test_scans = [scan + '\n' for scan in sorted(list(test_scans))]

    n_scans = len(train_scans) + len(val_scans) + len(test_scans)
    logs.append(f'n_scans: {n_scans}\n')
    if split_name == "standard":
        logs.append(f'n_neg_scans:n_pos_scans = {(n_scans - n_scans_with_roosts) / n_scans_with_roosts}\n')
    logs.append(f'n_train_scans: {len(train_scans)}\n')
    logs.append(f'n_val_scans: {len(val_scans)}\n')
    logs.append(f'n_test_scans: {len(test_scans)}\n\n')

    logs.append(
        f'n_days: '
        f'{SPLITS[split_name]["n_train_days"] + SPLITS[split_name]["n_val_days"] + SPLITS[split_name]["n_test_days"]}\n'
    )
    logs.append(f'n_train_days: {SPLITS[split_name]["n_train_days"]}\n')
    logs.append(f'n_val_days: {SPLITS[split_name]["n_val_days"]}\n')
    logs.append(f'n_test_days: {SPLITS[split_name]["n_test_days"]}\n\n')

    with open(os.path.join(SPLITS[split_name]["dir"], 'train.txt'), "w") as f:
        f.writelines(train_scans)
    with open(os.path.join(SPLITS[split_name]["dir"], 'val.txt'), "w") as f:
        f.writelines(val_scans)
    with open(os.path.join(SPLITS[split_name]["dir"], 'test.txt'), "w") as f:
        f.writelines(test_scans)
    with open(os.path.join(SCAN_LIST_DIR, 'stats.txt'), "w") as f:
        f.writelines(logs)